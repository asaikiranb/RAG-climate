"""
LLM-judged generation quality metrics: faithfulness, relevance, completeness.
Uses local Ollama for scoring (no rate limits).
"""

import json
import re
import requests
from typing import Dict


OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2:latest"


JUDGE_PROMPT = """You are an expert evaluator of RAG (Retrieval-Augmented Generation) systems.

Given a QUESTION, the CONTEXT (retrieved sources), and the ANSWER generated by the system, 
score the answer on these three dimensions from 0.0 to 1.0:

1. **Faithfulness**: Is the answer factually grounded in the provided context? 
   - 1.0 = every claim is supported by the context
   - 0.0 = the answer contains fabricated or unsupported information

2. **Relevance**: Does the answer directly address the question?
   - 1.0 = fully addresses the question
   - 0.0 = completely off-topic

3. **Completeness**: Does the answer cover the key aspects needed to fully answer the question?
   - 1.0 = comprehensive and thorough
   - 0.0 = missing most important points

Return ONLY a JSON object with exactly these keys: faithfulness, relevance, completeness
Each value must be a float between 0.0 and 1.0.
Do not include any other text or explanation.

QUESTION:
{question}

CONTEXT:
{context}

ANSWER:
{answer}

JSON:"""


def judge_generation(
    question: str,
    context: str,
    answer: str,
    **kwargs,  # accepts groq_client etc. for backward compat, ignored
) -> Dict[str, float]:
    """
    Use local Ollama to judge faithfulness, relevance, and completeness.
    Returns dict with keys: faithfulness, relevance, completeness, overall.
    """
    prompt = JUDGE_PROMPT.format(
        question=question,
        context=context[:4000],  # truncate to stay within limits
        answer=answer,
    )

    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "system": "You are a precise evaluator. Return only valid JSON.",
                "stream": False,
                "options": {
                    "temperature": 0.0,
                    "num_predict": 200,
                },
            },
            timeout=120,
        )
        response.raise_for_status()

        raw = response.json().get("response", "").strip()

        # Try to extract JSON from the response
        json_match = re.search(r'\{[^}]+\}', raw)
        if json_match:
            scores = json.loads(json_match.group())
        else:
            scores = json.loads(raw)

        faithfulness = float(scores.get("faithfulness", 0.0))
        relevance = float(scores.get("relevance", 0.0))
        completeness = float(scores.get("completeness", 0.0))

        # Clamp to [0, 1]
        faithfulness = max(0.0, min(1.0, faithfulness))
        relevance = max(0.0, min(1.0, relevance))
        completeness = max(0.0, min(1.0, completeness))

        overall = round((faithfulness + relevance + completeness) / 3, 4)

        return {
            "faithfulness": round(faithfulness, 4),
            "relevance": round(relevance, 4),
            "completeness": round(completeness, 4),
            "overall": overall,
        }

    except Exception as e:
        print(f"    âš  Generation scoring error: {e}")
        return {
            "faithfulness": 0.0,
            "relevance": 0.0,
            "completeness": 0.0,
            "overall": 0.0,
        }
